{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BOlo4Ctmtm4p"
      },
      "source": [
        "# Transfer Learning Assignment\n",
        "반갑습니다 여러분. 과제를 맡은 김준호입니다. TL 세션에서 다룬 Fine-tuning과 Domain Adaptation을 직접 구현해 봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWlAlAeRtm4s"
      },
      "source": [
        "Office-31은 여러 실습에서 자주 등장하는 이미지 데이터셋입니다. Amazon, Webcam, DSLR 세 개의 도메인으로 구성되어 있는데요. \n",
        "\n",
        "총 31개 클래스 (키보드, 마우스, 모니터 등)의 사무용품 이미지가 있고,각 도메인마다 같은 클래스가 포함되어 있지만 도메인마다 이미지 특성은 다릅니다. \n",
        "\n",
        "그래서 이런 transfer learning 실습에 적합하다고 볼 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6R098oQTS_TC"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import torch\n",
        "import torchvision\n",
        "from torchvision import transforms, datasets\n",
        "import torch.nn as nn\n",
        "import time\n",
        "from torchvision import models\n",
        "torch.cuda.set_device(0)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K9Ut17hwUDq-"
      },
      "source": [
        "amazon을 source로, webcam을 target data로 이용해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j4e--IIRU68M"
      },
      "outputs": [],
      "source": [
        "data_folder = 'OFFICE31'\n",
        "batch_size = 32\n",
        "n_class = 31\n",
        "domain_src, domain_tar = 'amazon', 'webcam'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XA6e2YaPtm4u"
      },
      "source": [
        "source와 target domain에 대한 DataLoader를 생성하고 load해 줍시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6JtN_bK9VFcM"
      },
      "outputs": [],
      "source": [
        "def load_data(root_path, domain, batch_size, phase):\n",
        "    transform_dict = {\n",
        "        'src': transforms.Compose(\n",
        "        [transforms.RandomResizedCrop(224),\n",
        "         transforms.RandomHorizontalFlip(),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ]),\n",
        "        'tar': transforms.Compose(\n",
        "        [transforms.Resize(224),\n",
        "         transforms.ToTensor(),\n",
        "         transforms.Normalize(mean=[0.485, 0.456, 0.406],\n",
        "                              std=[0.229, 0.224, 0.225]),\n",
        "         ])}\n",
        "    data = datasets.ImageFolder(root=os.path.join(root_path, domain), transform=transform_dict[phase])\n",
        "    data_loader = torch.utils.data.DataLoader(data, batch_size=batch_size, shuffle=phase=='src', drop_last=phase=='tar', num_workers=4)\n",
        "    return data_loader"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Jf_Gw2HRVJM_"
      },
      "outputs": [],
      "source": [
        "src_loader = load_data(data_folder, domain_src, batch_size, phase='src')\n",
        "tar_loader = load_data(data_folder, domain_tar, batch_size, phase='tar')\n",
        "print(f'Source data number: {len(src_loader.dataset)}')\n",
        "print(f'Target data number: {len(tar_loader.dataset)}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "caKMn438tm4v"
      },
      "source": [
        "# Fine-tuning"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Pre-trained ResNet50을 기반으로 한 간단한 TransferModel을 정의합니다. 여기서 모델 구조는 유지하되 마지막 fc layer만 새롭게 학습되도록 구성합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OXAjmY7pVK8t"
      },
      "outputs": [],
      "source": [
        "class TransferModel(nn.Module):\n",
        "    def __init__(self,\n",
        "                base_model : str = 'resnet50',\n",
        "                pretrain : bool = True,\n",
        "                n_class : int = 31):\n",
        "        super(TransferModel, self).__init__()\n",
        "        self.base_model = base_model\n",
        "        self.pretrain = pretrain\n",
        "        self.n_class = n_class\n",
        "        if self.base_model == 'resnet50':\n",
        "            self.model = # TODO: pre-trained model 불러오기\n",
        "            n_features = self.model.fc.in_features\n",
        "            fc = # TODO: 새로운 fc layer를 정의\n",
        "            # TODO: 모델의 fc layer를 새로운 fc로 교체\n",
        "        else:\n",
        "            # Use other models you like, such as vgg or alexnet\n",
        "            pass\n",
        "        self.model.fc.weight.data.normal_(0, 0.005)\n",
        "        self.model.fc.bias.data.fill_(0.1)\n",
        "\n",
        "    def forward(self, x):\n",
        "        return self.model(x)\n",
        "    \n",
        "    def predict(self, x):\n",
        "        return self.forward(x)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UAqT-0jRtm4w"
      },
      "source": [
        "모델이 정상적으로 작동하는지 random tensor로 테스트해 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "LewRmYIvXEIo"
      },
      "outputs": [],
      "source": [
        "model = TransferModel().cuda()\n",
        "RAND_TENSOR = torch.randn(1, 3, 224, 224).cuda()\n",
        "output = model(RAND_TENSOR)\n",
        "print(output)\n",
        "print(output.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UpX-fuiXtm4w"
      },
      "source": [
        "## Finetune ResNet-50"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0uNeAiPatm4w"
      },
      "source": [
        "Office-31 dataset은 validation set이 따로 없으므로, validation set으로 target domain을 이용해 줍시다.\n",
        "\n",
        "fine-tuning을 위한 학습 및 평가 함수를 정의합니다.\n",
        "학습은 source domain에서 수행하고 target domain에서 검증합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h74gKIVqtm4w"
      },
      "outputs": [],
      "source": [
        "dataloaders = {'src': src_loader,\n",
        "               'val': tar_loader,\n",
        "               'tar': tar_loader}\n",
        "n_epoch = 100\n",
        "criterion = nn.CrossEntropyLoss()\n",
        "early_stop = 20"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "-fz_FlAIXsTF"
      },
      "outputs": [],
      "source": [
        "def finetune(model, dataloaders, optimizer):\n",
        "    since = time.time()\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    for epoch in range(0, n_epoch):\n",
        "        stop += 1\n",
        "        # You can uncomment this line for scheduling learning rate\n",
        "        # lr_schedule(optimizer, epoch)\n",
        "        for phase in ['src', 'val', 'tar']:\n",
        "            if phase == 'src':\n",
        "                model.train()\n",
        "            else:\n",
        "                model.eval()\n",
        "            total_loss, correct = 0, 0\n",
        "            for inputs, labels in dataloaders[phase]:\n",
        "                inputs, labels = inputs.cuda(), labels.cuda()\n",
        "                optimizer.zero_grad()\n",
        "                with torch.set_grad_enabled(phase == 'src'):\n",
        "                    outputs = model(inputs)\n",
        "                    loss = criterion(outputs, labels)\n",
        "                preds = torch.max(outputs, 1)[1]\n",
        "                if phase == 'src':\n",
        "                    loss.backward()\n",
        "                    optimizer.step()\n",
        "                total_loss += loss.item() * inputs.size(0)\n",
        "                correct += torch.sum(preds == labels.data)\n",
        "            epoch_loss = total_loss / len(dataloaders[phase].dataset)\n",
        "            epoch_acc = correct.double() / len(dataloaders[phase].dataset)\n",
        "            print(f'Epoch: [{epoch:02d}/{n_epoch:02d}]---{phase}, loss: {epoch_loss:.6f}, acc: {epoch_acc:.4f}')\n",
        "            if phase == 'val' and epoch_acc > best_acc:\n",
        "                stop = 0\n",
        "                best_acc = epoch_acc\n",
        "                torch.save(model.state_dict(), 'model.pkl')\n",
        "        if stop >= early_stop:\n",
        "            break\n",
        "        print()\n",
        "   \n",
        "    time_pass = time.time() - since\n",
        "    print(f'Training complete in {time_pass // 60:.0f}m {time_pass % 60:.0f}s')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rGUZGrm7tm4x"
      },
      "source": [
        "이제 학습 파라미터들과 optimizer를 정의합니다.\n",
        "간단하게 SGD optimizer를 사용하고 fc layer의 학습률을 다른 layer보다 10배 크게 설정합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HGVIu0JXZaGT"
      },
      "outputs": [],
      "source": [
        "param_group = []\n",
        "learning_rate = 0.0001\n",
        "momentum = 5e-4\n",
        "for k, v in model.named_parameters():\n",
        "    if not k.__contains__('fc'):\n",
        "        param_group += [{'params': v, 'lr': learning_rate}]\n",
        "    else:\n",
        "        param_group += [{'params': v, 'lr': learning_rate * 10}]\n",
        "optimizer = torch.optim.SGD(param_group, momentum=momentum)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F7zGR_PFtm4y"
      },
      "source": [
        "## Train and test"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uKKrah-AZsHt"
      },
      "outputs": [],
      "source": [
        "# TODO: fine-tuning function 호출하여 학습 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GR5Y1x4btm4y"
      },
      "outputs": [],
      "source": [
        "def test(model, target_test_loader):\n",
        "    model.eval()\n",
        "    correct = 0\n",
        "    len_target_dataset = len(target_test_loader.dataset)\n",
        "    with torch.no_grad():\n",
        "        for data, target in target_test_loader:\n",
        "            data, target = data.cuda(), target.cuda()\n",
        "            s_output = model.predict(data)\n",
        "            pred = torch.max(s_output, 1)[1]\n",
        "            correct += torch.sum(pred == target)\n",
        "    acc = correct.double() / len(target_test_loader.dataset)\n",
        "    return acc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "1O2CBxmxtm4y"
      },
      "outputs": [],
      "source": [
        "model.load_state_dict(torch.load('model.pkl'))\n",
        "acc_test = test(model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "I7Iuk_Mitm4z"
      },
      "source": [
        "여기까지가 fine-tuning 파트입니다. 실제 학습에서는 learning rate decay 같은 기법도 사용하지만, 이 과제에서는 그것이 핵심이 아니므로 생략합니다.\n",
        "\n",
        "이제 같은 dataloader를 그대로 활용해서 domain adaptation 실험을 이어가봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "bO4c_QcGtm4z"
      },
      "source": [
        "# Domain Adaptation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VJwcwLQftm40"
      },
      "source": [
        "Domain adaptation의 핵심 구조는 fine-tuning과 매우 비슷하지만,\n",
        "두 도메인 간 분포 차이를 줄이기 위한 loss function을 추가해야 합니다.\n",
        "\n",
        "여기서는 MMD와 Coral loss를 사용해 두 도메인 간 분포 차이를 계산하는 loss function을 정의합니다. \n",
        "\n",
        "해당 loss를 이용할 수 있도록 새로운 모델 클래스를 정의하고 source의 feature와 label, 그리고 target feature를 모두 이용하도록 학습 스크립트를 수정해봅시다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jy_1xwdJtm40"
      },
      "source": [
        "### Loss function\n",
        "Domain Adaptation에서 가장 많이 사용되는 손실 함수는 MMD (Maximum Mean Discrepancy)입니다.\n",
        "\n",
        "비교를 위해 또 다른 대표적인 손실 함수인 CORAL (CORrelation ALignment)도 함께 살펴봅니다."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z3-wKorUtm40"
      },
      "source": [
        "#### MMD loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MpQH6VFwtm41"
      },
      "outputs": [],
      "source": [
        "class MMD_loss(nn.Module):\n",
        "    def __init__(self, kernel_type='rbf', kernel_mul=2.0, kernel_num=5):\n",
        "        super(MMD_loss, self).__init__()\n",
        "        self.kernel_num = kernel_num\n",
        "        self.kernel_mul = kernel_mul\n",
        "        self.fix_sigma = None\n",
        "        self.kernel_type = kernel_type\n",
        "\n",
        "    def guassian_kernel(self, source, target, kernel_mul=2.0, kernel_num=5, fix_sigma=None):\n",
        "        n_samples = int(source.size()[0]) + int(target.size()[0])\n",
        "        total = torch.cat([source, target], dim=0)\n",
        "        total0 = total.unsqueeze(0).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        total1 = total.unsqueeze(1).expand(\n",
        "            int(total.size(0)), int(total.size(0)), int(total.size(1)))\n",
        "        L2_distance = ((total0-total1)**2).sum(2)\n",
        "        if fix_sigma:\n",
        "            bandwidth = fix_sigma\n",
        "        else:\n",
        "            bandwidth = torch.sum(L2_distance.data) / (n_samples**2-n_samples)\n",
        "        bandwidth /= kernel_mul ** (kernel_num // 2)\n",
        "        bandwidth_list = [bandwidth * (kernel_mul**i)\n",
        "                          for i in range(kernel_num)]\n",
        "        kernel_val = [torch.exp(-L2_distance / bandwidth_temp)\n",
        "                      for bandwidth_temp in bandwidth_list]\n",
        "        return sum(kernel_val)\n",
        "\n",
        "    def linear_mmd2(self, f_of_X, f_of_Y):\n",
        "        loss = 0.0\n",
        "        delta = f_of_X.float().mean(0) - f_of_Y.float().mean(0)\n",
        "        loss = delta.dot(delta.T)\n",
        "        return loss\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        if self.kernel_type == 'linear':\n",
        "            return self.linear_mmd2(source, target)\n",
        "        elif self.kernel_type == 'rbf':\n",
        "            batch_size = int(source.size()[0])\n",
        "            kernels = self.guassian_kernel(\n",
        "                source, target, kernel_mul=self.kernel_mul, kernel_num=self.kernel_num, fix_sigma=self.fix_sigma)\n",
        "            XX = torch.mean(kernels[:batch_size, :batch_size])\n",
        "            YY = torch.mean(kernels[batch_size:, batch_size:])\n",
        "            XY = torch.mean(kernels[:batch_size, batch_size:])\n",
        "            YX = torch.mean(kernels[batch_size:, :batch_size])\n",
        "            loss = torch.mean(XX + YY - XY - YX)\n",
        "            return loss\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NcfUy_2Dtm41"
      },
      "source": [
        "#### CORAL loss"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "uZhKJq15tm41"
      },
      "outputs": [],
      "source": [
        "def CORAL(source, target):\n",
        "    d = source.size(1)\n",
        "    ns, nt = source.size(0), target.size(0)\n",
        "\n",
        "    # source covariance\n",
        "    tmp_s = torch.ones((1, ns)).cuda() @ source\n",
        "    cs = (source.t() @ source - (tmp_s.t() @ tmp_s) / ns) / (ns - 1)\n",
        "\n",
        "    # target covariance\n",
        "    tmp_t = torch.ones((1, nt)).cuda() @ target\n",
        "    ct = (target.t() @ target - (tmp_t.t() @ tmp_t) / nt) / (nt - 1)\n",
        "\n",
        "    # frobenius norm\n",
        "    loss = (cs - ct).pow(2).sum().sqrt()\n",
        "    loss = loss / (4 * d * d)\n",
        "\n",
        "    return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VB2cDp8Gtm41"
      },
      "source": [
        "### Model\n",
        "여기서도 backbone으로는 ResNet-50을 사용합니다.\n",
        "다만 이번에는 ResNet-50의 마지막 classifier layer를 제거한 feature extractor로 사용합니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UOLx_OSxtm41"
      },
      "outputs": [],
      "source": [
        "from torchvision import models\n",
        "class ResNet50Fc(nn.Module):\n",
        "    def __init__(self):\n",
        "        super(ResNet50Fc, self).__init__()\n",
        "        model_resnet50 = models.resnet50(pretrained=True)\n",
        "        self.conv1 = model_resnet50.conv1\n",
        "        self.bn1 = model_resnet50.bn1\n",
        "        self.relu = model_resnet50.relu\n",
        "        self.maxpool = model_resnet50.maxpool\n",
        "        self.layer1 = model_resnet50.layer1\n",
        "        self.layer2 = model_resnet50.layer2\n",
        "        self.layer3 = model_resnet50.layer3\n",
        "        self.layer4 = model_resnet50.layer4\n",
        "        self.avgpool = model_resnet50.avgpool\n",
        "        self.__in_features = model_resnet50.fc.in_features\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.conv1(x)\n",
        "        x = self.bn1(x)\n",
        "        x = self.relu(x)\n",
        "        x = self.maxpool(x)\n",
        "        x = self.layer1(x)\n",
        "        x = self.layer2(x)\n",
        "        x = self.layer3(x)\n",
        "        x = self.layer4(x)\n",
        "        x = self.avgpool(x)\n",
        "        x = x.view(x.size(0), -1)\n",
        "        return x\n",
        "\n",
        "    def output_num(self):\n",
        "        return self.__in_features"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IRgdNM3Wtm42"
      },
      "source": [
        "이제 Domain Adaptation을 위한 핵심 모델 클래스를 정의합니다.\n",
        "\n",
        "ResNet-50을 기반으로 하되, bottleneck layer와 새로운 fc layer를 추가합니다.\n",
        "\n",
        "중요한 점은 adapt_loss 함수로 우리가 정의한 MMD 또는 CORAL loss를 forward pass에서 함께 계산한다는 것입니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oC5NKJpJtm42"
      },
      "outputs": [],
      "source": [
        "class TransferNet(nn.Module):\n",
        "    def __init__(self,\n",
        "                 num_class, \n",
        "                 base_net='resnet50', \n",
        "                 transfer_loss='mmd', \n",
        "                 use_bottleneck=True, \n",
        "                 bottleneck_width=256, \n",
        "                 width=1024):\n",
        "        super(TransferNet, self).__init__()\n",
        "        if base_net == 'resnet50':\n",
        "            # TODO: ResNet50 기반 feature extractor를 정의\n",
        "        else:\n",
        "            # Your own basenet\n",
        "            return\n",
        "        self.use_bottleneck = use_bottleneck\n",
        "        self.transfer_loss = transfer_loss\n",
        "        bottleneck_list = # TODO: bottleneck layer를 정의\n",
        "        self.bottleneck_layer = nn.Sequential(*bottleneck_list)\n",
        "        classifier_layer_list = # TODO: classifier layer를 정의\n",
        "        self.classifier_layer = nn.Sequential(*classifier_layer_list)\n",
        "\n",
        "        self.bottleneck_layer[0].weight.data.normal_(0, 0.005)\n",
        "        self.bottleneck_layer[0].bias.data.fill_(0.1)\n",
        "        for i in range(2):\n",
        "            self.classifier_layer[i * 3].weight.data.normal_(0, 0.01)\n",
        "            self.classifier_layer[i * 3].bias.data.fill_(0.0)\n",
        "\n",
        "    def forward(self, source, target):\n",
        "        source = self.base_network(source)\n",
        "        target = self.base_network(target)\n",
        "        source_clf = self.classifier_layer(source)\n",
        "        if self.use_bottleneck:\n",
        "            source = self.bottleneck_layer(source)\n",
        "            target = self.bottleneck_layer(target)\n",
        "        transfer_loss = self.adapt_loss(source, target, self.transfer_loss)\n",
        "        return source_clf, transfer_loss\n",
        "\n",
        "    def predict(self, x):\n",
        "        features = self.base_network(x)\n",
        "        clf = self.classifier_layer(features)\n",
        "        return clf\n",
        "\n",
        "    def adapt_loss(self, X, Y, adapt_loss):\n",
        "        \"\"\"Compute adaptation loss, currently we support mmd and coral\n",
        "\n",
        "        Arguments:\n",
        "            X {tensor} -- source matrix\n",
        "            Y {tensor} -- target matrix\n",
        "            adapt_loss {string} -- loss type, 'mmd' or 'coral'. You can add your own loss\n",
        "\n",
        "        Returns:\n",
        "            [tensor] -- adaptation loss tensor\n",
        "        \"\"\"\n",
        "        if adapt_loss == 'mmd':\n",
        "            mmd_loss = MMD_loss()\n",
        "            loss = mmd_loss(X, Y)\n",
        "        elif adapt_loss == 'coral':\n",
        "            loss = CORAL(X, Y)\n",
        "        else:\n",
        "            # Your own loss\n",
        "            loss = 0\n",
        "        return loss"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hAdPs27btm42"
      },
      "source": [
        "### Train\n",
        "이제 Domain Adaptation 모델을 학습시켜 봅시다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OK6P8uMDtm42"
      },
      "outputs": [],
      "source": [
        "transfer_loss = 'mmd'\n",
        "learning_rate = 0.0001\n",
        "transfer_model = TransferNet(n_class, transfer_loss=transfer_loss, base_net='resnet50').cuda()\n",
        "optimizer = torch.optim.SGD([\n",
        "    {'params': transfer_model.base_network.parameters()},\n",
        "    {'params': transfer_model.bottleneck_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "    {'params': transfer_model.classifier_layer.parameters(), 'lr': 10 * learning_rate},\n",
        "], lr=learning_rate, momentum=0.9, weight_decay=5e-4)\n",
        "lamb = 10 # weight for transfer loss, it is a hyperparameter that needs to be tuned"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4WpUfcHItm42"
      },
      "source": [
        "학습 함수에서는 source 데이터와 label, target 데이터를 모두 사용해야 하므로, source와 target의 dataloader를 zip으로 묶어서 동시에 iterate합니다.\n",
        "\n",
        "보통 두 도메인의 샘플 수는 다르기 때문에, 여러 epoch에 걸쳐 무작위로 잘 섞이면 전체 데이터를 충분히 학습할 수 있습니다."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AGlVNI2ktm42"
      },
      "outputs": [],
      "source": [
        "def train(dataloaders, model, optimizer):\n",
        "    source_loader, target_train_loader, target_test_loader = dataloaders['src'], dataloaders['val'], dataloaders['tar']\n",
        "    len_source_loader = len(source_loader)\n",
        "    len_target_loader = len(target_train_loader)\n",
        "    best_acc = 0\n",
        "    stop = 0\n",
        "    n_batch = min(len_source_loader, len_target_loader)\n",
        "    for e in range(n_epoch):\n",
        "        stop += 1\n",
        "        train_loss_clf, train_loss_transfer, train_loss_total = 0, 0, 0\n",
        "        model.train()\n",
        "        for (src, tar) in zip(source_loader, target_train_loader):\n",
        "            data_source, label_source = src\n",
        "            data_target, _ = tar\n",
        "            data_source, label_source = data_source.cuda(), label_source.cuda()\n",
        "            data_target = data_target.cuda()\n",
        "\n",
        "            optimizer.zero_grad()\n",
        "            label_source_pred, transfer_loss = model(data_source, data_target)\n",
        "            clf_loss = criterion(label_source_pred, label_source)\n",
        "            loss = clf_loss + lamb * transfer_loss\n",
        "            loss.backward()\n",
        "            optimizer.step()\n",
        "            train_loss_clf = clf_loss.detach().item() + train_loss_clf\n",
        "            train_loss_transfer = transfer_loss.detach().item() + train_loss_transfer\n",
        "            train_loss_total = loss.detach().item() + train_loss_total\n",
        "        acc = test(model, target_test_loader)\n",
        "        print(f'Epoch: [{e:2d}/{n_epoch}], cls_loss: {train_loss_clf/n_batch:.4f}, transfer_loss: {train_loss_transfer/n_batch:.4f}, total_Loss: {train_loss_total/n_batch:.4f}, acc: {acc:.4f}')\n",
        "        if best_acc < acc:\n",
        "            best_acc = acc\n",
        "            torch.save(model.state_dict(), 'trans_model.pkl')\n",
        "            stop = 0\n",
        "        if stop >= early_stop:\n",
        "            break"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nqSCG6-Xtm43",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# TODO: Domain Adaptation 모델을 학습시키는 함수를 호출하여 학습 진행"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yWvYODRDtm43"
      },
      "outputs": [],
      "source": [
        "transfer_model.load_state_dict(torch.load('trans_model.pkl'))\n",
        "acc_test = test(transfer_model, dataloaders['tar'])\n",
        "print(f'Test accuracy: {acc_test}')"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "collapsed_sections": [],
      "name": "deep_transfer_tutorial.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "newenv",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.9.21"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
